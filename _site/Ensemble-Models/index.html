<!DOCTYPE html>
<html>
  <head>
    <title>Ensemble Learning Models – Thoughts of a Data Scientist – Blog about my learnings in Data Science field</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the  problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.

" />
    <meta property="og:description" content="Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the  problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.

" />
    
    <meta name="author" content="Thoughts of a Data Scientist" />

    
    <meta property="og:title" content="Ensemble Learning Models" />
    <meta property="twitter:title" content="Ensemble Learning Models" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Thoughts of a Data Scientist - Blog about my learnings in Data Science field" href="/feed.xml" />
	
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
	<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Ensemble Learning Models | Thoughts of a Data Scientist</title>
<meta property="og:title" content="Ensemble Learning Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models." />
<meta property="og:description" content="Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models." />
<link rel="canonical" href="http://localhost:4000/Ensemble-Models/" />
<meta property="og:url" content="http://localhost:4000/Ensemble-Models/" />
<meta property="og:site_name" content="Thoughts of a Data Scientist" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-13T00:00:00+05:30" />
<script type="application/ld+json">
{"name":null,"description":"Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.","author":null,"@type":"BlogPosting","url":"http://localhost:4000/Ensemble-Models/","publisher":null,"image":null,"headline":"Ensemble Learning Models","dateModified":"2017-11-13T00:00:00+05:30","datePublished":"2017-11-13T00:00:00+05:30","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Ensemble-Models/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Thoughts of a Data Scientist</a></h1>
            <p class="site-description">Blog about my learnings in Data Science field</p>
          </div>

          <nav>
            <a href="/portfolio">Portfolio</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Ensemble Learning Models</h1>

  <div class="entry">
    <p>Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the  problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.</p>

<p>Random forests is an ensemble learning method that builds a large number of decision trees that are weak classifiers and then combine them to build a stable and strong classifier that is better than the individual trees created in the forest.This falls under the axiom that “whole is greater than the sum of its parts”.</p>

<p>Some of the widely used ensemble learning techniques are</p>

<h1 id="bagging"><a href="#header-1"></a>Bagging</h1>

<p>Bagging stands for <strong>B</strong>ootstrap <strong>Agg</strong>regat<strong>ing</strong> which means different training subsets are randomly drawn, with replacement from  the entire training datasets.
Each training data subset is used to train a different classifier of a same type and then all the individual classifiers are combined for a final decision.</p>
<h1 id="boosting"><a href="#header-1"></a>Boosting</h1>

<p>Boosting is a two step approach, in which we first uses subsets of the original data to produce a series of averagely performing models and then “boosts” their performance by combining them using a particular cost function. Unlike Bagging , subset creation is not random it depends upon the performance of previous models used.XGBoost,GBM,AdaBoost are all ensemble models.</p>
<h1 id="stacking"><a href="#header-1"></a>Stacking</h1>

<p>In stacking, multiple layers of machine learning models are placed one over another where each of the model passes their predictions to the model in the layer above it and the top-layer model takes decision based on the output of the models in layers below it.</p>


  </div>

  <div class="date">
    Written on November 13, 2017
  </div>

  
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:sakshi.manga@gmail.com"><i class="svg-icon email"></i></a>


<a href="https://github.com/imsakshi"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/sakshi-manga-0ab1a7a6"><i class="svg-icon linkedin"></i></a>


<a href="https://www.twitter.com/DSsakshi"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    

  </body>
</html>
