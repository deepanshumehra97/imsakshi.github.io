<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-11-14T23:03:25+05:30</updated><id>http://localhost:4000/</id><title type="html">Thoughts of a Data Scientist</title><subtitle>Blog about my learnings in Data Science field</subtitle><entry><title type="html">Ensemble Learning Models</title><link href="http://localhost:4000/Ensemble-Models/" rel="alternate" type="text/html" title="Ensemble Learning Models" /><published>2017-11-13T00:00:00+05:30</published><updated>2017-11-13T00:00:00+05:30</updated><id>http://localhost:4000/Ensemble-Models</id><content type="html" xml:base="http://localhost:4000/Ensemble-Models/">&lt;p&gt;Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the  problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.&lt;/p&gt;

&lt;p&gt;Random forests is an ensemble learning method that builds a large number of decision trees that are weak classifiers and then combine them to build a stable and strong classifier that is better than the individual trees created in the forest.This falls under the axiom that “whole is greater than the sum of its parts”.&lt;/p&gt;

&lt;p&gt;Some of the widely used ensemble learning techniques are&lt;/p&gt;

&lt;h1 id=&quot;bagging&quot;&gt;&lt;a href=&quot;#header-1&quot;&gt;&lt;/a&gt;Bagging&lt;/h1&gt;

&lt;p&gt;Bagging stands for &lt;strong&gt;B&lt;/strong&gt;ootstrap &lt;strong&gt;Agg&lt;/strong&gt;regat&lt;strong&gt;ing&lt;/strong&gt; which means different training subsets are randomly drawn, with replacement from  the entire training datasets.
Each training data subset is used to train a different classifier of a same type and then all the individual classifiers are combined for a final decision.&lt;/p&gt;
&lt;h1 id=&quot;boosting&quot;&gt;&lt;a href=&quot;#header-1&quot;&gt;&lt;/a&gt;Boosting&lt;/h1&gt;

&lt;p&gt;Boosting is a two step approach, in which we first uses subsets of the original data to produce a series of averagely performing models and then “boosts” their performance by combining them using a particular cost function. Unlike Bagging , subset creation is not random it depends upon the performance of previous models used.XGBoost,GBM,AdaBoost are all ensemble models.&lt;/p&gt;
&lt;h1 id=&quot;stacking&quot;&gt;&lt;a href=&quot;#header-1&quot;&gt;&lt;/a&gt;Stacking&lt;/h1&gt;

&lt;p&gt;In stacking, multiple layers of machine learning models are placed one over another where each of the model passes their predictions to the model in the layer above it and the top-layer model takes decision based on the output of the models in layers below it.&lt;/p&gt;</content><author><name></name></author><summary type="html">Ensemble Learning is a technique that combines two or more algorithms of similar or dissimilar types called Base or weak learners. In contrast to ML, Ensemble learning allows to train multiple learners at a same time to solve the problem. Ensemble methods are an excellent way to improve predictive performance and are widely used on Kaggle platform by Data Scientists to improve accuracy and performance of their models.</summary></entry></feed>